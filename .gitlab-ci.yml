image: eicweb.phy.anl.gov:4567/containers/eic_container/jug_xl:nightly

variables:
  JOBS: 1

default:
  timeout: 3 hours
  artifacts:
    expire_in: 1 week
    paths:
      - results/

stages:
  - gzip
  - nevents
  - timings
  - collect

.gzip:
  stage: gzip
  script:
    - mc config host add S3rw https://dtn01.sdcc.bnl.gov:9000 ${S3RW_ACCESS_KEY} ${S3RW_SECRET_KEY}
    - echo $DATA
    - cat $DATA
    - cat $DATA | grep -v "^\#" | parallel -j $JOBS --colsep "," scripts/gzip_hepmc.sh {1} {2} {3} {4}

.nevents:
  stage: nevents
  script:
    - mc config host add S3 https://dtn01.sdcc.bnl.gov:9000 ${S3_ACCESS_KEY} ${S3_SECRET_KEY}
    - mkdir -p $(dirname results/datasets/nevents/$DATA)
    - echo $DATA
    - cat $DATA
    - cat $DATA | grep -v "^\#" | parallel -k --lb -j $JOBS --colsep "," scripts/count_events.sh results/datasets/nevents/$DATA {1} {2} {3} {4}
    - sort -o results/datasets/nevents/$DATA results/datasets/nevents/$DATA
    - sort -o $DATA $DATA
    - diff $DATA results/datasets/nevents/$DATA
.timings:
  stage: timings
  script:
    - mc config host add S3 https://dtn01.sdcc.bnl.gov:9000 ${S3_ACCESS_KEY} ${S3_SECRET_KEY}
    - mkdir -p $(dirname results/datasets/timings/$DATA)
    - echo $DATA
    # Use sed '1!d1' instead of head -n 1 to avoid pipefail issues
    - cat results/datasets/nevents/$DATA | grep -v "^\#" | sed '1!d'
    - cat results/datasets/nevents/$DATA | grep -v "^\#" | sed '1!d' | parallel -k --lb -j $JOBS --colsep "," scripts/determine_timing.sh results/datasets/timings/$DATA {1} {2} {3} {4}
    - echo $DATA
    - cat results/datasets/timings/$DATA
    - |
      IFS="," read file ext nevents dt0 dt1 < results/datasets/timings/$DATA
      export dt0 dt1
      cat results/datasets/nevents/$DATA | grep -v "^\#" | cat       | parallel -k --lb -j $JOBS --colsep "," scripts/determine_timing.sh results/datasets/timings/$DATA {1} {2} {3} {4}
    - sort -o results/datasets/timings/$DATA results/datasets/timings/$DATA

.timings_all:
  stage: timings
  script:
    - mc config host add S3 https://dtn01.sdcc.bnl.gov:9000 ${S3_ACCESS_KEY} ${S3_SECRET_KEY}
    - mkdir -p $(dirname results/datasets/timings/$DATA)
    - echo $DATA
    - cat results/datasets/nevents/$DATA | grep -v "^\#"
    - cat results/datasets/nevents/$DATA | grep -v "^\#" | parallel -k --lb -j $JOBS --colsep "," scripts/determine_timing.sh results/datasets/timings/$DATA {1} {2} {3} {4}
    - sort -o results/datasets/timings/$DATA results/datasets/timings/$DATA

.collect:
  stage: collect
  script:
    - rm -rf results/logs/
    - find results/datasets/
    - find results/datasets/timings/ -name "*.csv" -print0 -exec awk 'BEGIN {FS=","} {sum+=$3*$5+$4} END {print(":",sum/3600,"core-hours")}' {} \;
    - for d in `find results/datasets/timings/ -type d` ; do
        echo -n $d ;
        find $d -name "*.csv" -exec cat {} \; | awk 'BEGIN {FS=","} {sum+=$3*$5+$4} END {print(":",sum/3600,"core-hours")}' ;
      done

include:
  - local: 'DIS/config.yml'
  - local: 'EXCLUSIVE/config.yml'
  - local: 'SIDIS/config.yml'
  - local: 'SINGLE/config.yml'
#  - local: 'SR/config.yml'

collect:
  extends: .collect
  needs:
    - "DIS:collect"
    - "EXCLUSIVE:collect"
    - "SIDIS:collect"
    - "SINGLE:collect"
#    - "SR:collect"
